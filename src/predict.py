"""
æµ‹è¯•é›†é¢„æµ‹æ¨¡å—
å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹å¹¶ç”Ÿæˆsubmission.csvæ–‡ä»¶
"""

import os
import warnings
import numpy as np
import torch
import pandas as pd
from torch.utils.data import Dataset, DataLoader
from torchvision.transforms import Resize
from tqdm import tqdm
from src.transforms import GWTransform

# æŠ‘åˆ¶è­¦å‘Š
os.environ["PYTHONWARNINGS"] = "ignore"
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
warnings.filterwarnings("ignore")


class TestDataset(Dataset):
    """
    æµ‹è¯•æ•°æ®é›†ç±»
    ç”¨äºåŠ è½½æµ‹è¯•é›†çš„.npyæ–‡ä»¶ï¼ˆä¸éœ€è¦æ ‡ç­¾ï¼‰
    """
    def __init__(self, file_paths):
        """
        åˆå§‹åŒ–æµ‹è¯•æ•°æ®é›†
        
        å‚æ•°:
            file_paths: æ•°æ®æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        self.file_paths = file_paths
        self.transform = GWTransform()  # CQTå˜æ¢
        self.resize = Resize((224, 224), antialias=True)  # è°ƒæ•´åˆ°EfficientNetæ ‡å‡†è¾“å…¥å°ºå¯¸

    def __len__(self):
        """è¿”å›æ•°æ®é›†å¤§å°"""
        return len(self.file_paths)

    def __getitem__(self, idx):
        """
        è·å–å•ä¸ªæ•°æ®æ ·æœ¬
        
        å‚æ•°:
            idx: æ ·æœ¬ç´¢å¼•
            
        è¿”å›:
            image: å¤„ç†åçš„å›¾åƒå¼ é‡ (3, 224, 224)
            file_id: æ–‡ä»¶IDï¼ˆä¸å«æ‰©å±•åï¼‰
        """
        # 1. åŠ è½½æ•°æ®
        path = self.file_paths[idx]
        waves = np.load(path)  # å½¢çŠ¶: (3, 4096) - 3ä¸ªæ¢æµ‹å™¨çš„æ—¶åŸŸä¿¡å·
        
        # 2. æå–æ–‡ä»¶IDï¼ˆä»è·¯å¾„ä¸­æå–ï¼Œä¾‹å¦‚ï¼š00005bced6ï¼‰
        file_id = os.path.splitext(os.path.basename(path))[0]
        
        # 3. è½¬æ¢ä¸ºå¼ é‡
        wave_tensor = torch.from_numpy(waves).float()
        
        # 4. å®‰å…¨å½’ä¸€åŒ–ï¼ˆæ¯ä¸ªé€šé“ç‹¬ç«‹è¿›è¡Œæœ€å°-æœ€å¤§å½’ä¸€åŒ–ï¼‰
        for i in range(3):  # å¯¹3ä¸ªæ¢æµ‹å™¨é€šé“åˆ†åˆ«å½’ä¸€åŒ–
            w_min = wave_tensor[i].min()
            w_max = wave_tensor[i].max()
            wave_tensor[i] = (wave_tensor[i] - w_min) / (w_max - w_min + 1e-8)

        # 5. CQTå˜æ¢å’Œå¯¹æ•°ç¼©æ”¾ï¼ˆæµ‹è¯•æ—¶ä¸ä½¿ç”¨æ•°æ®å¢å¼ºï¼‰
        image = self.transform(wave_tensor, training=False)
        
        # 6. è°ƒæ•´å¤§å°åˆ°224x224ï¼ˆEfficientNetæ ‡å‡†è¾“å…¥å°ºå¯¸ï¼‰
        image = self.resize(image)
        
        # 7. å›¾åƒæœ€ç»ˆå½’ä¸€åŒ–ï¼ˆ0åˆ°1èŒƒå›´ï¼‰
        img_min = image.min()
        img_max = image.max()
        image = (image - img_min) / (img_max - img_min + 1e-8)
        
        return image, file_id


def get_file_path(data_dir, file_id):
    """
    æ ¹æ®æ–‡ä»¶IDæ„å»ºæ–‡ä»¶è·¯å¾„
    åˆ©ç”¨ç›®å½•ç»“æ„è§„å¾‹ï¼šæ–‡ä»¶IDçš„å‰3ä¸ªå­—ç¬¦åˆ†åˆ«å¯¹åº”3å±‚ç›®å½•ç»“æ„
    
    å‚æ•°:
        data_dir: æ•°æ®ç›®å½•æ ¹è·¯å¾„
        file_id: æ–‡ä»¶IDï¼ˆä¸å«æ‰©å±•åï¼‰
        
    è¿”å›:
        å®Œæ•´çš„æ–‡ä»¶è·¯å¾„
        
    ç¤ºä¾‹:
        get_file_path("/data", "21000bb588") -> "/data/2/1/0/21000bb588.npy"
    """
    if len(file_id) < 3:
        raise ValueError(f"File ID must be at least 3 characters: {file_id}")
    return os.path.join(data_dir, file_id[0], file_id[1], file_id[2], f"{file_id}.npy")


def find_test_files(test_dir):
    """
    æ‰«ææµ‹è¯•ç›®å½•ï¼Œæ‰¾åˆ°æ‰€æœ‰.npyæ–‡ä»¶
    ä¼˜åŒ–ï¼šé™åˆ¶os.walkæ·±åº¦ä¸º3å±‚ï¼Œå› ä¸ºæ–‡ä»¶éƒ½åœ¨3å±‚ç›®å½•ä¸‹
    
    å‚æ•°:
        test_dir: æµ‹è¯•ç›®å½•è·¯å¾„
        
    è¿”å›:
        file_paths: æ‰€æœ‰.npyæ–‡ä»¶çš„è·¯å¾„åˆ—è¡¨
        file_ids: å¯¹åº”çš„æ–‡ä»¶IDåˆ—è¡¨
    """
    file_paths = []
    file_ids = []
    
    print(f"æ‰«ææµ‹è¯•ç›®å½•: {test_dir}")
    print("ä¼˜åŒ–ï¼šåªéå†3å±‚æ·±åº¦çš„ç›®å½•ï¼ˆæ–‡ä»¶éƒ½åœ¨3å±‚ç›®å½•ä¸‹ï¼‰")
    
    # è®¡ç®—æ ¹ç›®å½•çš„æ·±åº¦ï¼ˆç”¨äºé™åˆ¶éå†æ·±åº¦ï¼‰   #å®é™…ä¸Šæ²¡ç”¨ï¼Œaiè¯¯ç”Ÿæˆçš„
    root_depth = len(os.path.normpath(test_dir).split(os.sep))
    max_depth = root_depth + 3  # åªéå†åˆ°3å±‚å­ç›®å½•
    
    for root, dirs, files in os.walk(test_dir):
        # è®¡ç®—å½“å‰ç›®å½•çš„æ·±åº¦
        current_depth = len(os.path.normpath(root).split(os.sep))
        
        # å¦‚æœè¾¾åˆ°æˆ–è¶…è¿‡3å±‚æ·±åº¦ï¼Œå¤„ç†æ–‡ä»¶ä½†ä¸ç»§ç»­æ·±å…¥éå†
        if current_depth >= max_depth:
            # æ¸…ç©ºdirsåˆ—è¡¨ï¼Œé˜²æ­¢os.walkç»§ç»­æ·±å…¥æ›´æ·±å±‚çš„ç›®å½•
            dirs[:] = []
        
        # å¤„ç†å½“å‰ç›®å½•ä¸­çš„æ–‡ä»¶ï¼ˆæ–‡ä»¶åœ¨3å±‚ç›®å½•ä¸‹ï¼Œæ‰€ä»¥å½“current_depth == max_depthæ—¶å¤„ç†ï¼‰
        for file in files:
            if file.endswith(".npy"):
                file_path = os.path.join(root, file)
                file_id = os.path.splitext(file)[0]  # æå–æ–‡ä»¶IDï¼ˆä¸å«æ‰©å±•åï¼‰
                file_paths.append(file_path)
                file_ids.append(file_id)
    
    print(f"æ‰¾åˆ° {len(file_paths)} ä¸ªæµ‹è¯•æ–‡ä»¶")
    return file_paths, file_ids


def predict_test_set(model, test_loader, device):
    """
    å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹
    
    å‚æ•°:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
        device: è®¡ç®—è®¾å¤‡
        
    è¿”å›:
        predictions: å­—å…¸ï¼Œé”®ä¸ºæ–‡ä»¶IDï¼Œå€¼ä¸ºé¢„æµ‹æ¦‚ç‡
    """
    model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    predictions = {}
    
    print("å¼€å§‹é¢„æµ‹...")
    with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è®¡ç®—ä»¥èŠ‚çœå†…å­˜
        for images, file_ids in tqdm(test_loader, desc="é¢„æµ‹ä¸­"):
            images = images.to(device)
            
            # å‰å‘ä¼ æ’­
            outputs = model(images).squeeze()
            
            # è½¬æ¢ä¸ºæ¦‚ç‡ï¼ˆä½¿ç”¨sigmoidï¼‰
            probs = torch.sigmoid(outputs).cpu().numpy()
            
            # å°†æ¦‚ç‡è½¬æ¢ä¸ºäºŒåˆ†ç±»æ ‡ç­¾ï¼ˆ0æˆ–1ï¼‰
            # é€šå¸¸ä½¿ç”¨0.5ä½œä¸ºé˜ˆå€¼
            preds = (probs >= 0.5).astype(int)
            
            # å­˜å‚¨é¢„æµ‹ç»“æœ
            for file_id, pred in zip(file_ids, preds):
                predictions[file_id] = pred
    
    return predictions


def generate_submission(predictions, output_path="submission.csv"):
    """
    ç”Ÿæˆsubmission.csvæ–‡ä»¶
    
    å‚æ•°:
        predictions: å­—å…¸ï¼Œé”®ä¸ºæ–‡ä»¶IDï¼Œå€¼ä¸ºé¢„æµ‹æ ‡ç­¾ï¼ˆ0æˆ–1ï¼‰
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    # æŒ‰æ–‡ä»¶IDæ’åºï¼ˆç¡®ä¿è¾“å‡ºé¡ºåºä¸€è‡´ï¼‰
    sorted_ids = sorted(predictions.keys())
    
    # åˆ›å»ºDataFrame
    df = pd.DataFrame({
        'id': sorted_ids,
        'target': [predictions[fid] for fid in sorted_ids]
    })
    
    # ä¿å­˜ä¸ºCSVæ–‡ä»¶
    df.to_csv(output_path, index=False)
    print(f"\nâœ… é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    print(f"å…±é¢„æµ‹ {len(df)} ä¸ªæ ·æœ¬")
    print(f"\nå‰5ä¸ªé¢„æµ‹ç»“æœé¢„è§ˆ:")
    print(df.head())


def main():
    """
    ä¸»å‡½æ•°ï¼šæ‰§è¡Œæµ‹è¯•é›†é¢„æµ‹æµç¨‹
    """
    import argparse
    from src.model import GWClassifier
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='æµ‹è¯•é›†é¢„æµ‹')
    parser.add_argument('--test_dir', type=str, 
                       default='/kaggle/input/g2net-gravitational-wave-detection/test',
                       help='æµ‹è¯•æ•°æ®ç›®å½•è·¯å¾„')
    parser.add_argument('--model_path', type=str,
                       default='models/best_model.pth',
                       help='æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', type=str,
                       default='submission.csv',
                       help='è¾“å‡ºCSVæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--batch_size', type=int,
                       default=32,
                       help='æ‰¹æ¬¡å¤§å°')
    
    args = parser.parse_args()
    
    # è®¾ç½®è®¡ç®—è®¾å¤‡
    if torch.backends.mps.is_available():
        device = torch.device("mps")
        print("ğŸš€ ä½¿ç”¨è®¾å¤‡: MacOS GPU (MPS)")
    elif torch.cuda.is_available():
        device = torch.device("cuda")
        print("ğŸš€ ä½¿ç”¨è®¾å¤‡: NVIDIA GPU (CUDA)")
    else:
        device = torch.device("cpu")
        print("âš ï¸ ä½¿ç”¨è®¾å¤‡: CPU")
    
    # 1. æ‰«ææµ‹è¯•æ–‡ä»¶
    print("\n[1/4] æ‰«ææµ‹è¯•æ–‡ä»¶...")
    if not os.path.exists(args.test_dir):
        print(f"âŒ é”™è¯¯: æµ‹è¯•ç›®å½•ä¸å­˜åœ¨: {args.test_dir}")
        return
    
    file_paths, file_ids = find_test_files(args.test_dir)
    
    if len(file_paths) == 0:
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ°ä»»ä½•.npyæ–‡ä»¶")
        return
    
    # 2. åˆ›å»ºæµ‹è¯•æ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    print("\n[2/4] åˆ›å»ºæµ‹è¯•æ•°æ®é›†...")
    test_dataset = TestDataset(file_paths)
    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)
    
    # 3. åŠ è½½æ¨¡å‹
    print(f"\n[3/4] åŠ è½½æ¨¡å‹: {args.model_path}")
    if not os.path.exists(args.model_path):
        print(f"âŒ é”™è¯¯: æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.model_path}")
        return
    
    model = GWClassifier(pretrained=False)  # ä¸éœ€è¦é¢„è®­ç»ƒæƒé‡ï¼Œå› ä¸ºè¦åŠ è½½å·²è®­ç»ƒçš„æƒé‡
    model.load_state_dict(torch.load(args.model_path, map_location=device))
    model.to(device)
    
    # 4. è¿›è¡Œé¢„æµ‹
    print("\n[4/4] è¿›è¡Œé¢„æµ‹...")
    predictions = predict_test_set(model, test_loader, device)
    
    # 5. ç”Ÿæˆsubmission.csv
    print("\nç”Ÿæˆsubmission.csvæ–‡ä»¶...")
    generate_submission(predictions, args.output)
    
    print("\nâœ… é¢„æµ‹å®Œæˆï¼")


if __name__ == "__main__":
    main()

# ç”Ÿæˆå›°éš¾çš„æ—¶å€™å¯ä»¥å°è¯•æ–°å»ºä¸ªtab