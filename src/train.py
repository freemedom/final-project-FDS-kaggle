"""
è®­ç»ƒæ¨¡å—
åŒ…å«å®Œæ•´çš„è®­ç»ƒå¾ªç¯ã€éªŒè¯ã€æ¢¯åº¦è£å‰ªã€å­¦ä¹ ç‡è°ƒåº¦ç­‰åŠŸèƒ½
"""

import torch
import torch.nn as nn
from tqdm import tqdm
import numpy as np
from sklearn.metrics import roc_auc_score
import time 

class Trainer:
    """
    å¼•åŠ›æ³¢åˆ†ç±»å™¨è®­ç»ƒå™¨
    åŒ…å«ï¼šè®­ç»ƒå¾ªç¯ã€éªŒè¯ã€æ¢¯åº¦è£å‰ªã€å­¦ä¹ ç‡è°ƒåº¦å’Œè®¡æ—¶å™¨
    """
    def __init__(self, model, train_loader, val_loader, device, lr=1e-4):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        å‚æ•°:
            model: è¦è®­ç»ƒçš„æ¨¡å‹
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            device: è®¡ç®—è®¾å¤‡ï¼ˆCPU/GPUï¼‰
            lr: å­¦ä¹ ç‡
        """
        self.model = model.to(device)
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.device = device
        
        # æŸå¤±å‡½æ•°ï¼šäºŒå…ƒäº¤å‰ç†µï¼ˆå¸¦logitsï¼Œæ•°å€¼ç¨³å®šï¼‰
        self.criterion = nn.BCEWithLogitsLoss()
        
        # ä¼˜åŒ–å™¨ï¼šAdamWæ˜¯EfficientNetçš„æ ‡å‡†é€‰æ‹©
        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr, weight_decay=1e-2)
        
        # æŒ‡æ ‡å’Œå†å²è®°å½•
        self.best_score = 0.0  # æœ€ä½³éªŒè¯AUCåˆ†æ•°
        self.history = {'train_loss': [], 'val_loss': [], 'train_auc': [], 'val_auc': []}

    def train_one_epoch(self):
        """
        è®­ç»ƒä¸€ä¸ªepoch
        
        è¿”å›:
            epoch_loss: å¹³å‡è®­ç»ƒæŸå¤±
            epoch_auc: è®­ç»ƒé›†AUCåˆ†æ•°
        """
        self.model.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
        running_loss = 0.0
        all_targets = []
        all_preds = []
        
        pbar = tqdm(self.train_loader, desc="Training", leave=False)
        
        for images, targets in pbar:
            images, targets = images.to(self.device), targets.to(self.device)
            
            # 1. æ¸…é›¶æ¢¯åº¦
            self.optimizer.zero_grad()
            
            # 2. å‰å‘ä¼ æ’­
            outputs = self.model(images).squeeze()
            loss = self.criterion(outputs, targets)
            
            # 3. åå‘ä¼ æ’­
            loss.backward()
            
            # 4. æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            # 5. ä¼˜åŒ–å™¨æ›´æ–°
            self.optimizer.step()
            
            # ç»Ÿè®¡ä¿¡æ¯
            running_loss += loss.item()
            preds = torch.sigmoid(outputs).detach().cpu().numpy()  # è½¬æ¢ä¸ºæ¦‚ç‡
            all_preds.extend(preds)
            all_targets.extend(targets.cpu().numpy())
            
            pbar.set_postfix({'loss': loss.item()})
            
        epoch_loss = running_loss / len(self.train_loader)
        try:
            epoch_auc = roc_auc_score(all_targets, all_preds)
        except:
            epoch_auc = 0.5  # å¦‚æœè®¡ç®—å¤±è´¥ï¼Œè¿”å›éšæœºçŒœæµ‹çš„AUC
            
        return epoch_loss, epoch_auc

    def evaluate(self):
        """
        åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹
        
        è¿”å›:
            avg_loss: å¹³å‡éªŒè¯æŸå¤±
            auc_score: éªŒè¯é›†AUCåˆ†æ•°
        """
        self.model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        running_loss = 0.0
        all_targets = []
        all_preds = []
        
        with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è®¡ç®—ä»¥èŠ‚çœå†…å­˜å’ŒåŠ é€Ÿ
            for images, targets in tqdm(self.val_loader, desc="Validation", leave=False):
                images, targets = images.to(self.device), targets.to(self.device)
                
                outputs = self.model(images).squeeze()
                loss = self.criterion(outputs, targets)
                
                running_loss += loss.item()
                preds = torch.sigmoid(outputs).cpu().numpy()
                all_preds.extend(preds)
                all_targets.extend(targets.cpu().numpy())
        
        avg_loss = running_loss / len(self.val_loader)
        try:
            auc_score = roc_auc_score(all_targets, all_preds)
        except:
            auc_score = 0.5  # å¦‚æœè®¡ç®—å¤±è´¥ï¼Œè¿”å›éšæœºçŒœæµ‹çš„AUC
            
        return avg_loss, auc_score

    def fit(self, epochs, save_path="models/best_model.pth"):
        """
        æ‰§è¡Œå®Œæ•´çš„è®­ç»ƒæµç¨‹
        
        å‚æ•°:
            epochs: è®­ç»ƒè½®æ•°
            save_path: æ¨¡å‹ä¿å­˜è·¯å¾„
            
        è¿”å›:
            history: åŒ…å«è®­ç»ƒå†å²çš„å­—å…¸
        """
        print(f"Starting training on {self.device}...")
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼šå½“éªŒè¯AUCä¸å†æå‡æ—¶é™ä½å­¦ä¹ ç‡
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='max', factor=0.5, patience=2  # è€å¿ƒå€¼2ï¼Œå› å­0.5
        )
        
        total_start = time.time()  # å¼€å§‹æ€»è®¡æ—¶å™¨
        
        for epoch in range(epochs):
            epoch_start = time.time()  # å¼€å§‹epochè®¡æ—¶å™¨
            
            print(f"\nEpoch {epoch+1}/{epochs}")
            
            # è®­ç»ƒå’ŒéªŒè¯
            train_loss, train_auc = self.train_one_epoch()
            val_loss, val_auc = self.evaluate()
            
            # è®°å½•å†å²
            self.history['train_loss'].append(train_loss)
            self.history['val_loss'].append(val_loss)
            self.history['train_auc'].append(train_auc)
            self.history['val_auc'].append(val_auc)
            
            # æ›´æ–°å­¦ä¹ ç‡
            scheduler.step(val_auc)
            
            # è®¡ç®—epochæ—¶é—´
            epoch_end = time.time()
            epoch_mins = int((epoch_end - epoch_start) / 60)
            epoch_secs = int((epoch_end - epoch_start) % 60)
            
            print(f"â±ï¸ Time: {epoch_mins}m {epoch_secs}s")
            print(f"Train Loss: {train_loss:.4f} | Train AUC: {train_auc:.4f}")
            print(f"Val Loss:   {val_loss:.4f} | Val AUC:   {val_auc:.4f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_auc > self.best_score:
                print(f"ğŸš€ Score Improved ({self.best_score:.4f} -> {val_auc:.4f}). Saving model...")
                self.best_score = val_auc
                torch.save(self.model.state_dict(), save_path)
            else:
                print("Score did not improve.")
        
        # è®¡ç®—æ€»è®­ç»ƒæ—¶é—´
        total_end = time.time()
        total_mins = int((total_end - total_start) / 60)
        total_secs = int((total_end - total_start) % 60)
        print(f"\nğŸ Total Training Time: {total_mins}m {total_secs}s")
        
        return self.history